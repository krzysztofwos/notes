By [[Jeremy England]]

<iframe width="560" height="315" src="https://www.youtube.com/embed/c4vvl39QpBQ?si=SzgagSXKFLYLTMT9" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

Absorbing energy from a time-varying field is one way of doing work.

Darwin says that if the diversity and morphology or function alters the ability of one or another of the self-replicating forms to make copies of itself, then the thing that is better at copying itself and can do so faster takes over in the future, and its traits are more represented.

Adaptation in terms of the physical properties of the system.

The statistical irreversibility of something happening has a direct quantitative relationship to the entropy that is produced in the surroundings. This relationship between statistical irreversibility and entropy production remains exact according to Newton's laws with an additional modification: the addition of a time-varying field (something that oscillates or modulates a system over time)

![["What is life" lecture by Jeremy England at the Karolinska Institutet 24-29 screenshot.png]]

As we do that, we can still talk about the probability of fluctuating from $i$ to $j$ and $j$ to $i$ because of thermal fluctuations, but the fact that we are driving the system now makes it so that every time that happens, it generates a different amount of heat in the surroundings. The reason is that now the work goes in from the drive, and we no longer have this conservation principle that we know the heat just because of the starting and ending energies of these systems.

So, if we average the exponential of this entropy production over all the possible ways it can happen, it still remains the case that the statistical irreversibility is related to entropy production.

We want to build this up to the macroscopic arrangements of the system because the microscopic arrangements of the system is not something we can measure. We want to coarse grain—collect together a bunch of different states and call them the same according to some kind of measurement or observation we can make about them.

![["What is life" lecture by Jeremy England at the Karolinska Institutet 26-28 screenshot.png]]

Once we group the states together, we can construct an arbitrary probability distribution over each set of states. We start with each set of states; we evolve with some arbitrary drive over a finite amount of time, and we end up with some different distribution of states.

![["What is life" lecture by Jeremy England at the Karolinska Institutet 26-36 screenshot.png]]

> Die Grenzen meiner Sprache bedeuten die Grenzen meiner Welt.
>
> — Ludwig Wittgenstein

The borders of my language determine the borders of my world.

In physics, we are accustomed to doing this coarse-graining in such a way that these states correspond to a measurable quantity—numbers that we assign to the system. But it doesn't have to be numbers. It can also be any kind of categorization.

Until we classify the states, we have not layered our description of the world on top of a more basic physical description.

If we have a macrostate of any kind and we are driving it in any way, and it's also in contact with a heat bath, and it's fluctuating and changing its shape over time, then we can define the forward and reverse transitions between these macroscopic descriptions, and if know these probabilities, they are still related to entropy production in the same way, averaging over all the possible ways things can happen, but now we have to look at total entropy, not just the heat that we exhaust into our surrounding.

Now, we also have to think about the organization of the internal parts of the system—how many different arrangements are there that would satisfy condition one, and how many different arrangements are there that would satisfy condition two?

![["What is life" lecture by Jeremy England at the Karolinska Institutet 29-59 screenshot.png]]

We generally think that the total entropy change $\Delta S_\text{total}$ has to be positive for a spontaneous process. Now we can talk about the statistical irreversibility of an arbitrary macroscopic transition, and the more irreversible it is—the more likely it is to see something run forward rather than in the reverse direction—the larger and more positive the lower bound on the total entropy production has to be.

![["What is life" lecture by Jeremy England at the Karolinska Institutet 30-39 screenshot.png]]

The [[Laundauer Bound]] is one special case of this more general statement.

![["What is life" lecture by Jeremy England at the Karolinska Institutet 33-48 screenshot.png]]

How about the choice between going to B or C, given that we start in A?

![["What is life" lecture by Jeremy England at the Karolinska Institutet 34-39 screenshot.png]]

We start in some arbitrary macrostate A, and after some finite amount of time, we can talk about generating a certain amount of entropy in the surroundings and going from A to B, or from A to C, we can break this probability apart into pieces.

![["What is life" lecture by Jeremy England at the Karolinska Institutet 35-15 screenshot.png]]

This is a generalization of [[Free Energy]] for an arbitrarily driven system evolving over a finite period of time under an arbitrary external driving field. If we want to know how much more likely we are to go to B than to C, we care about the following things:

1. The order or organization difference between B and C. All things being equal, it is easier to do something disorganized rather than organized. All things being equal, you're under pressure to be disorganized.
2. Durability. A kinetic term that does not appear in equilibrium thermodynamics. We are more likely to go from A to B if we are also more likely to go from B to A.
3. Fluctuation and dissipation. We are trying to reliably increase the dissipation.

Adaptation. We are defining a time-varying environment, and we want to know how the structure that emerges reflects a property of that environment. Why is it that this structure forms and persists as opposed to another one?

We have a general tendency towards the discovery of organized states, where the story of how they form is that they absorb a lot of work from the environment, and they cause a lot of dissipation in the surroundings as a result.

Emerging computation. If you have an environment that fluctuates where there are correlations that make it so that things that happened to you in the past allow you to make sensible predictions of what is likely to happen to you in the future, then one of the things you are driven to is an organized structure that seems capable to predict future outcomes in the environment.
