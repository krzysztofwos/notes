Performance of LLMs is a smooth, well-behaved, predictable function of:

- **N**, the number of parameters in the network
- **D**, the amount of text we train on
  And the trends do not show signs of "topping out"
  â‡’ We can expect more intelligence "for free" by scaling

![[Training Compute-Optimal Large Language Models Figure 4.png]]

https://www.youtube.com/watch?v=zjkBMFhNj_g&t=1543s
